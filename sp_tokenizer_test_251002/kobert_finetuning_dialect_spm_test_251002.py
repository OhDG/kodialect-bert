# ==============================================================================
# ### 0ë‹¨ê³„: í™˜ê²½ ì„¤ì •
# ==============================================================================
import os
import json
import re
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoModelForSequenceClassification,
    XLMRobertaTokenizer,      # âœ… 'ëŠë¦°' í† í¬ë‚˜ì´ì €ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    PreTrainedTokenizerFast,  # âœ… 'ë¹ ë¥¸' í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.utils.class_weight import compute_class_weight
from collections import Counter

print(f"PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}")

# ==============================================================================
# ### 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© (Fine-tuningìš©)
# ==============================================================================
# (ë°ì´í„° ë¡œë”© ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼í•˜ë©°, ìƒëµí•©ë‹ˆë‹¤. í•„ìš”ì‹œ ì´ì „ ì½”ë“œ ë‚´ìš©ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.)
print("===== [1ë‹¨ê³„] Fine-tuningìš© ì‚¬íˆ¬ë¦¬ ë°ì´í„° ë° ë¼ë²¨ ë¡œë”© ì‹œì‘ =====")

# ì§€ì—­ë³„ í´ë”ëª…ê³¼ ë¼ë²¨ ì •ì˜
region_dirs = {
    "ê°•ì›ë„": "JSONë§Œ_ëª¨ì€í´ë”_ê°•ì›ë„", "ê²½ìƒë„": "JSONë§Œ_ëª¨ì€í´ë”_ê²½ìƒë„",
    "ì „ë¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì „ë¼ë„", "ì œì£¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì œì£¼ë„",
    "ì¶©ì²­ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì¶©ì²­ë„"
}
region_label = {"ê°•ì›ë„": 0, "ê²½ìƒë„": 1, "ì „ë¼ë„": 2, "ì œì£¼ë„": 3, "ì¶©ì²­ë„": 4}
file_dir = "../../project1_dataset"

all_texts, all_labels = [], []

for region, subdir in region_dirs.items():
    dir_path = os.path.join(file_dir, subdir)
    if not os.path.exists(dir_path): continue
    for filename in os.listdir(dir_path):
        if filename.endswith(".json"):
            file_path = os.path.join(dir_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                    utterances = data.get("utterance", [])
                    for u in utterances:
                        text = u.get("dialect_form", "")
                        if isinstance(text, str) and text.strip():
                            cleaned_text = re.sub(r'\([^)]*\)|\[[^)]*\]', '', text)
                            cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9.,?! ]', '', cleaned_text).strip()
                            if cleaned_text:
                                all_texts.append(cleaned_text)
                                all_labels.append(region_label[region])
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {e}")

df_all = pd.DataFrame({"text": all_texts, "label": all_labels})
train_df, test_df = train_test_split(df_all, test_size=0.2, stratify=df_all["label"], random_state=42)
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))
print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: Train {len(train_dataset):,}ê°œ, Test {len(test_dataset):,}ê°œ")


# # ==============================================================================
# # ### 2ë‹¨ê³„: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ì¤€ë¹„ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# # ==============================================================================
# print("\n===== [2ë‹¨ê³„] ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹œì‘ =====")

# # âœ… 1. SentencePieceë¡œ ë§Œë“  .model íŒŒì¼ì„ ì§ì ‘ ë¡œë“œí•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
# #    (ì´ì „ì— ìƒì„±í•œ 'dialect_spm.model' íŒŒì¼ì´ ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)
# tokenizer_file = "dialect_spm.model"
# tokenizer = PreTrainedTokenizerFast(
#     tokenizer_file=tokenizer_file,
#     pad_token="<pad>", # PAD í† í° ëª…ì‹œ
#     unk_token="<unk>", # UNK í† í° ëª…ì‹œ
#     bos_token="<s>",   # BOS í† í° ëª…ì‹œ
#     eos_token="</s>",   # EOS í† í° ëª…ì‹œ
# )
# print(f"ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ. ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")


# # âœ… 2. KoBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
# model = AutoModelForSequenceClassification.from_pretrained(
#     "monologg/kobert",
#     num_labels=5
# )
# print(f"ê¸°ì¡´ KoBERT ëª¨ë¸ì˜ ì„ë² ë”© í¬ê¸°: {model.get_input_embeddings().weight.shape[0]}")



# # âœ… 3. (ë§¤ìš° ì¤‘ìš”!) ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸°ë¥¼ ìƒˆë¡œìš´ í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
# #    ì´ ê³¼ì •ì€ ê¸°ì¡´ ì„ë² ë”© ì§€ì‹ì„ ë¬´íš¨í™”í•˜ê³ , ìƒˆë¡œìš´ í¬ê¸°ì— ë§ì¶° ë ˆì´ì–´ë¥¼ ì‚¬ì‹¤ìƒ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
# model.resize_token_embeddings(len(tokenizer))
# print(f"ë¦¬ì‚¬ì´ì¦ˆëœ ëª¨ë¸ì˜ ì„ë² ë”© í¬ê¸°: {model.get_input_embeddings().weight.shape[0]}")


# ==============================================================================
# ### 2ë‹¨ê³„: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ì¤€ë¹„ (ìµœì¢… ìˆ˜ì •)
# ==============================================================================
print("\n===== [2ë‹¨ê³„] ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹œì‘ =====")

# âœ… 1. 'ëŠë¦°' í† í¬ë‚˜ì´ì €ë¡œ .modelê³¼ .vocab íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
#    XLMRobertaTokenizerëŠ” sentencepiece ëª¨ë¸ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ 'ëŠë¦°' í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.
# slow_tokenizer = XLMRobertaTokenizer(vocab_file="dialect_spm.vocab", sp_model_file="dialect_spm.model")


slow_tokenizer = XLMRobertaTokenizer(vocab_file="dialect_spm.model")

# âœ… 2. (ì¤‘ìš”) BERT ê³„ì—´ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” íŠ¹ìˆ˜ í† í°ë“¤ì„ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.
special_tokens_to_add = ['[CLS]', '[SEP]', '[MASK]']
slow_tokenizer.add_special_tokens({'additional_special_tokens': special_tokens_to_add})

# âœ… 3. 'ëŠë¦°' í† í¬ë‚˜ì´ì €ë¥¼ 'ë¹ ë¥¸' í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#    ì´ì œ ì´ tokenizer ê°ì²´ëŠ” ë‹¤ë¥¸ ë¹ ë¥¸ í† í¬ë‚˜ì´ì €ì²˜ëŸ¼ ë™ì‘í•©ë‹ˆë‹¤.
tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=slow_tokenizer,
    pad_token="<pad>",
    unk_token="<unk>",
    bos_token="<s>",
    eos_token="</s>",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
print(f"ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ë³€í™˜ ì™„ë£Œ. ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")

# âœ… 4. KoBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
model = AutoModelForSequenceClassification.from_pretrained(
    "monologg/kobert",
    num_labels=5
)
print(f"ê¸°ì¡´ KoBERT ëª¨ë¸ì˜ ì„ë² ë”© í¬ê¸°: {model.get_input_embeddings().weight.shape[0]}")

# âœ… 5. (ë§¤ìš° ì¤‘ìš”!) ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸°ë¥¼ ìƒˆë¡œìš´ í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.
model.resize_token_embeddings(len(tokenizer))
print(f"ë¦¬ì‚¬ì´ì¦ˆëœ ëª¨ë¸ì˜ ì„ë² ë”© í¬ê¸°: {model.get_input_embeddings().weight.shape[0]}")


# ==============================================================================
# ### 3ë‹¨ê³„: ë°ì´í„° í† í°í™”
# ==============================================================================
# (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)
print("\n===== [3ë‹¨ê³„] ë°ì´í„°ì…‹ í† í°í™” ì‹œì‘ =====")

def tokenize_fn(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
tokenized_test = test_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

# ==============================================================================
# ### 4ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì ìš© Trainer ë° í•™ìŠµ ì„¤ì •
# ==============================================================================
print("\n===== [4ë‹¨ê³„] Trainer ì„¤ì • ì‹œì‘ =====")

class WeightedTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels"); outputs = model(**inputs); logits = outputs.get("logits")
        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device)); loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

train_labels = np.array(train_dataset['label'])
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

def compute_metrics(p):
    pred, labels = p; pred = np.argmax(pred, axis=1); accuracy = accuracy_score(y_true=labels, y_pred=pred)
    return {"accuracy": accuracy}

training_args = TrainingArguments(
    output_dir="./results_replace_tokenizer", num_train_epochs=20, per_device_train_batch_size=64,
    per_device_eval_batch_size=128, learning_rate=5e-5, warmup_steps=3000, weight_decay=0.01,
    logging_dir="./logs_replace_tokenizer", logging_steps=10000, evaluation_strategy="steps",
    eval_steps=10000, save_total_limit=2, save_steps=10000, load_best_model_at_end=True,
    metric_for_best_model="accuracy", greater_is_better=True, report_to="tensorboard",
)

trainer = WeightedTrainer(
    model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test,
    compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
    class_weights=class_weights_tensor
)

# ==============================================================================
# ### 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
# ==============================================================================
print("\n===== [5ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘ =====")
trainer.train()

model_save_path = "./my_best_dialect_model_replace_tokenizer"
trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"ìµœì  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ '{model_save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ==============================================================================
# ### 6ë‹¨ê³„: ìµœì¢… í‰ê°€ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ==============================================================================
print("\n===== [6ë‹¨ê³„] ìµœì¢… í‰ê°€ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘ =====")
predictions = trainer.predict(tokenized_test)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids
accuracy = accuracy_score(true_labels, pred_labels)
print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

print("\n--- ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---")
model_path = "./my_best_dialect_model_replace_tokenizer"
loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)
loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)
loaded_model.eval()

sentence = "ê±°ì‹œê¸° ì €ì§ ê°€ë³´ë‘ê»˜"; id2region = {0: "ê°•ì›ë„", 1: "ê²½ìƒë„", 2: "ì „ë¼ë„", 3: "ì œì£¼ë„", 4: "ì¶©ì²­ë„"}
inputs = loaded_tokenizer(sentence, return_tensors="pt")
with torch.no_grad():
    outputs = loaded_model(**inputs); logits = outputs.logits; probs = F.softmax(logits, dim=1)
    pred_label = torch.argmax(probs, dim=1).item()
print(f"ğŸ—£ ì…ë ¥ ë¬¸ì¥: {sentence}"); print(f"ğŸ“ ì˜ˆì¸¡ ì§€ì—­: {id2region[pred_label]} (í™•ë¥ : {probs[0][pred_label].item():.2%})")