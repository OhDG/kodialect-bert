import os
import json
import re
import pandas as pd
import sentencepiece as spm
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.utils.class_weight import compute_class_weight

print(f"PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}")

# ==============================================================================
# ### 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© (Fine-tuningìš©)
# ==============================================================================
print("===== [1ë‹¨ê³„] Fine-tuningìš© ì‚¬íˆ¬ë¦¬ ë°ì´í„° ë° ë¼ë²¨ ë¡œë”© ì‹œì‘ =====")
region_dirs = {
    "ê°•ì›ë„": "JSONë§Œ_ëª¨ì€í´ë”_ê°•ì›ë„",
    "ê²½ìƒë„": "JSONë§Œ_ëª¨ì€í´ë”_ê²½ìƒë„",
    "ì „ë¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì „ë¼ë„",
    "ì œì£¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì œì£¼ë„",
    "ì¶©ì²­ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì¶©ì²­ë„"
}
region_label = {"ê°•ì›ë„": 0, "ê²½ìƒë„": 1, "ì „ë¼ë„": 2, "ì œì£¼ë„": 3, "ì¶©ì²­ë„": 4}
file_dir = "../../project1_dataset"
all_texts, all_labels = [], []

for region, subdir in region_dirs.items():
    dir_path = os.path.join(file_dir, subdir)
    if not os.path.exists(dir_path):
        continue
    for filename in os.listdir(dir_path):
        if filename.endswith(".json"):
            file_path = os.path.join(dir_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                    utterances = data.get("utterance", [])
                    for u in utterances:
                        text = u.get("dialect_form", "")
                        if isinstance(text, str) and text.strip():
                            cleaned_text = re.sub(r'\([^)]*\)|\[[^)]*\]', '', text)
                            cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9.,?! ]', '', cleaned_text).strip()
                            if cleaned_text:
                                all_texts.append(cleaned_text)
                                all_labels.append(region_label[region])
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {e}")
            
            # âœ…âœ…âœ… L35: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1ê°œ íŒŒì¼ë§Œ ë¡œë“œ
            print(f"    -> {region} 1ê°œ íŒŒì¼ ë¡œë”© ì™„ë£Œ.")
            break

df_all = pd.DataFrame({"text": all_texts, "label": all_labels})
# (ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ stratifyê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ 0ê°œì—¬ë„ ì§„í–‰)
if len(df_all) > 10:
    train_df, test_df = train_test_split(df_all, test_size=0.2, stratify=df_all["label"], random_state=42)
else:
    train_df = df_all
    test_df = df_all.copy() # ê·¸ëƒ¥ ë³µì‚¬
    
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))
print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: Train {len(train_dataset):,}ê°œ, Test {len(test_dataset):,}ê°œ")

# ==============================================================================
# ### 2ë‹¨ê³„: ì»¤ìŠ¤í…€ "í™•ì¥" ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
# ==============================================================================
print("\n===== [2ë‹¨ê³„] Script 1ì—ì„œ ì €ì¥í•œ í™•ì¥ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”© =====")
extended_model_path = "./KoBERT-Extended-with-Dialect" 
tokenizer = AutoTokenizer.from_pretrained(extended_model_path)
model = AutoModelForSequenceClassification.from_pretrained(
    extended_model_path,
    num_labels=5,
    ignore_mismatched_sizes=True
)
print(f"í™•ì¥ëœ ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
print(f"í™•ì¥ëœ í† í¬ë‚˜ì´ì € ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")
print(f"ëª¨ë¸ì˜ ë¶„ë¥˜ Head í¬ê¸°: {model.num_labels}")

# ==============================================================================
# ### 3ë‹¨ê³„: ë°ì´í„° í† í°í™” (ë° ê²€ì¦)
# ==============================================================================
print("\n===== [3ë‹¨ê³„] ë°ì´í„°ì…‹ í† í°í™” ì‹œì‘ =====")
def tokenize_fn(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
tokenized_test = test_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

print("\n===== [3.5ë‹¨ê³„] í† í° ID ë° ì„ë² ë”© í¬ê¸° ê²€ì¦ =====")
emb_size = model.get_input_embeddings().num_embeddings
print(f"ëª¨ë¸ì˜ ì‹¤ì œ ì„ë² ë”© í¬ê¸° (num_embeddings): {emb_size}")
print(f"í† í¬ë‚˜ì´ì € ì–´íœ˜ ì‚¬ì „ í¬ê¸° (len): {len(tokenizer)}")

if emb_size != len(tokenizer):
    print(f"â€¼ï¸â€¼ï¸â€¼ï¸ ì˜¤ë¥˜: ëª¨ë¸ ì„ë² ë”© í¬ê¸°({emb_size})ì™€ í† í¬ë‚˜ì´ì € í¬ê¸°({len(tokenizer)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
    print("Script 1ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
    exit()

def max_id(ds):
    try: return max(max(row) for row in ds['input_ids'])
    except ValueError: return 0 

max_train_id = max_id(tokenized_train)
max_test_id = max_id(tokenized_test)
print(f"Max token ID in Train data: {max_train_id}")
print(f"Max token ID in Test data:  {max_test_id}")

if max_train_id >= emb_size or max_test_id >= emb_size:
    print(f"ğŸ’¥ğŸ’¥ğŸ’¥ ì˜¤ë¥˜: ë°ì´í„°ì˜ í† í° ID ({max(max_train_id, max_test_id)})ê°€ ëª¨ë¸ ì„ë² ë”© í¬ê¸° ({emb_size})ë³´ë‹¤ í½ë‹ˆë‹¤.")
    print("ì´ê²ƒì´ CUDA assert ì˜¤ë¥˜ì˜ ì›ì¸ì…ë‹ˆë‹¤. Script 1ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
    exit()
else:
    print("âœ… ê²€ì¦ í†µê³¼: ëª¨ë“  í† í° IDê°€ ëª¨ë¸ ì„ë² ë”© í¬ê¸°ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")

# ==============================================================================
# ### 4ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì ìš© Trainer ë° í•™ìŠµ ì„¤ì •
# ==============================================================================
print("\n===== [4ë‹¨ê³„] Trainer ì„¤ì • ì‹œì‘ =====")
class WeightedTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss
train_labels = np.array(train_dataset['label'])
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    return {"accuracy": accuracy_score(y_true=labels, y_pred=pred)}

# âœ…âœ…âœ… L161: ì‘ì€ ë°ì´í„°ì…‹ì— ë§ê²Œ íŒŒë¼ë¯¸í„° ì¡°ì •
training_args = TrainingArguments(
    output_dir="./results_custom_tokenizer", 
    num_train_epochs=5, # âœ… Epochs 20 -> 5 (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    per_device_train_batch_size=64, 
    per_device_eval_batch_size=128,
    learning_rate=5e-5, 
    warmup_steps=50, # âœ… 300 -> 50
    weight_decay=0.01,
    logging_dir="./logs_custom_tokenizer", 
    logging_steps=25, # âœ… 100 -> 25 (ì•½ 1 epochë§ˆë‹¤)
    evaluation_strategy="steps", 
    eval_steps=25, # âœ… 100 -> 25 (ì•½ 1 epochë§ˆë‹¤)
    save_total_limit=2,
    save_steps=25, # âœ… 100 -> 25
    load_best_model_at_end=True,
    metric_for_best_model="accuracy", 
    greater_is_better=True,
    report_to="tensorboard",
)
trainer = WeightedTrainer(
    model=model, args=training_args,
    train_dataset=tokenized_train, eval_dataset=tokenized_test,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
    class_weights=class_weights_tensor
)

# ==============================================================================
# ### 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
# ==============================================================================
print("\n===== [5ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘ =====")
trainer.train()

# ==============================================================================
# ### 6ë‹¨ê³„: ìµœì¢… í‰ê°€ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ==============================================================================
print("\n===== [6ë‹¨ê³„] ìµœì¢… í‰ê°€ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘ =====")
predictions = trainer.predict(tokenized_test)
pred_labels = np.argmax(predictions.predictions, axis=1)
accuracy = accuracy_score(predictions.label_ids, pred_labels)
print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")

print("\n--- ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---")
model_save_path = "./my_best_dialect_model_custom_tokenizer"
trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path)

loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)
loaded_model = AutoModelForSequenceClassification.from_pretrained(model_save_path)
loaded_model.eval()

sentence = "ê±°ì‹œê¸° ì €ì§ ê°€ë³´ë‘ê»˜"
id2region = {0: "ê°•ì›ë„", 1: "ê²½ìƒë„", 2: "ì „ë¼ë„", 3: "ì œì£¼ë„", 4: "ì¶©ì²­ë„"}
inputs = loaded_tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)
with torch.no_grad():
    outputs = loaded_model(**inputs)
    logits = outputs.logits
    probs = F.softmax(logits, dim=1)
    pred_label = torch.argmax(probs, dim=1).item()
print(f"ğŸ—£ ì…ë ¥ ë¬¸ì¥: {sentence}")
print(f"ğŸ“ ì˜ˆì¸¡ ì§€ì—­: {id2region[pred_label]} (í™•ë¥ : {probs[0][pred_label].item():.2%})")