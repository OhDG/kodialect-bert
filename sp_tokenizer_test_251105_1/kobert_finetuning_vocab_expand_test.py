# ==============================================================================
# ### 0ë‹¨ê³„: í™˜ê²½ ì„¤ì •
# ==============================================================================
import os
import json
import re
import pandas as pd
import sentencepiece as spm
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback
from sklearn.metrics import accuracy_score
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.utils.class_weight import compute_class_weight
from collections import Counter

print(f"PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}")

# ==============================================================================
# ### 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© (Fine-tuningìš©)
# ==============================================================================
print("===== [1ë‹¨ê³„] Fine-tuningìš© ì‚¬íˆ¬ë¦¬ ë°ì´í„° ë° ë¼ë²¨ ë¡œë”© ì‹œì‘ =====")

# ì§€ì—­ë³„ í´ë”ëª…ê³¼ ë¼ë²¨ ì •ì˜
region_dirs = {
    "ê°•ì›ë„": "JSONë§Œ_ëª¨ì€í´ë”_ê°•ì›ë„",
    "ê²½ìƒë„": "JSONë§Œ_ëª¨ì€í´ë”_ê²½ìƒë„",
    "ì „ë¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì „ë¼ë„",
    "ì œì£¼ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì œì£¼ë„",
    "ì¶©ì²­ë„": "JSONë§Œ_ëª¨ì€í´ë”_ì¶©ì²­ë„"
}
region_label = {"ê°•ì›ë„": 0, "ê²½ìƒë„": 1, "ì „ë¼ë„": 2, "ì œì£¼ë„": 3, "ì¶©ì²­ë„": 4}
file_dir = "../../project1_dataset"

all_texts = []
all_labels = []

# ê° ì§€ì—­ ë””ë ‰í† ë¦¬ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ëª¨ë“  JSON íŒŒì¼ ì½ê¸°
for region, subdir in region_dirs.items():
    dir_path = os.path.join(file_dir, subdir)
    if not os.path.exists(dir_path):
        continue
    for filename in os.listdir(dir_path):
        if filename.endswith(".json"):
            file_path = os.path.join(dir_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                    utterances = data.get("utterance", [])
                    # ë°ì´í„° í´ë¦¬ë‹ ì¶”ê°€
                    for u in utterances:
                        text = u.get("dialect_form", "")
                        if isinstance(text, str) and text.strip():
                            # ì •ê·œ í‘œí˜„ì‹ í´ë¦¬ë‹
                            cleaned_text = re.sub(r'\([^)]*\)|\[[^)]*\]', '', text)
                            cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9.,?! ]', '', cleaned_text).strip()
                            if cleaned_text:
                                all_texts.append(cleaned_text)
                                all_labels.append(region_label[region])
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì˜¤ë¥˜ ë°œìƒ: {file_path} - {e}")

            # âœ… ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë¬´ì‹œ
            break

# ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° train/test ë¶„ë¦¬
df_all = pd.DataFrame({"text": all_texts, "label": all_labels})
train_df, test_df = train_test_split(df_all, test_size=0.2, stratify=df_all["label"], random_state=42)
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: Train {len(train_dataset):,}ê°œ, Test {len(test_dataset):,}ê°œ")

# ==============================================================================
# ### 2ë‹¨ê³„: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ì¤€ë¹„ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# ==============================================================================
print("\n===== [2ë‹¨ê³„] ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ë¡œë”© ë° ëª¨ë¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹œì‘ =====")

# âœ… 1. ê¸°ë³¸ KoBERT í† í¬ë‚˜ì´ì €ë¥¼ ë¨¼ì € ë¡œë“œí•©ë‹ˆë‹¤.
# tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")

tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1", use_fast=True)
print(f"ê¸°ì¡´ KoBERT í† í¬ë‚˜ì´ì € ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")

# âœ… 2. SentencePieceë¡œ ë§Œë“  .vocab íŒŒì¼ì—ì„œ ìƒˆë¡œìš´ ë‹¨ì–´ë“¤ì„ ì½ì–´ì˜µë‹ˆë‹¤.
#    (ì´ì „ì— ìƒì„±í•œ 'dialect_spm.vocab' íŒŒì¼ì´ ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)
new_tokens = []
vocab_file_path = "dialect_spm.vocab" # SentencePieceë¡œ ë§Œë“  vocab íŒŒì¼
with open(vocab_file_path, 'r', encoding='utf-8') as f:
    for line in f:
        token = line.strip().split('\t')[0]
        if token not in tokenizer.get_vocab():
            new_tokens.append(token)

# âœ… 3. ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì— ìƒˆë¡œìš´ ë‹¨ì–´ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
tokenizer.add_tokens(new_tokens)
print(f"ìƒˆë¡œìš´ ì‚¬íˆ¬ë¦¬ í† í° {len(new_tokens)}ê°œ ì¶”ê°€ ì™„ë£Œ!")
print(f"í™•ì¥ëœ í† í¬ë‚˜ì´ì € ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(tokenizer)}")


# âœ… 4. KoBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
# model = AutoModelForSequenceClassification.from_pretrained(
#     "monologg/kobert",
#     num_labels=5
# )
model = AutoModelForSequenceClassification.from_pretrained("skt/kobert-base-v1", num_labels=5)


# âœ… 5. ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸°ë¥¼ í™•ì¥ëœ í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤. (ë§¤ìš° ì¤‘ìš”!)
model.resize_token_embeddings(len(tokenizer))
print("ëª¨ë¸ì˜ Token Embedding ë ˆì´ì–´ ë¦¬ì‚¬ì´ì¦ˆ ì™„ë£Œ!")


# ==============================================================================
# ### 3ë‹¨ê³„: ë°ì´í„° í† í°í™”
# ==============================================================================
print("\n===== [3ë‹¨ê³„] ë°ì´í„°ì…‹ í† í°í™” ì‹œì‘ =====")

def tokenize_fn(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128) # max_length ì¡°ì • ê°€ëŠ¥

tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])
tokenized_test = test_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

# ==============================================================================
# ### 4ë‹¨ê³„: ê°€ì¤‘ì¹˜ ì ìš© Trainer ë° í•™ìŠµ ì„¤ì •
# ==============================================================================
print("\n===== [4ë‹¨ê³„] Trainer ì„¤ì • ì‹œì‘ =====")

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•œ Trainer ì •ì˜
class WeightedTrainer(Trainer):
    def __init__(self, class_weights, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

# ê°€ì¤‘ì¹˜ ê³„ì‚°
train_labels = np.array(train_dataset['label'])
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

# í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    return {"accuracy": accuracy}

# TrainingArguments ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results_custom_tokenizer",
    num_train_epochs=20,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=128,
    learning_rate=5e-5,
    warmup_steps=3000,
    weight_decay=0.01,
    logging_dir="./logs_custom_tokenizer",
    logging_steps=10000,
    evaluation_strategy="steps",
    eval_steps=10000,
    save_total_limit=2,
    save_steps=10000,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    report_to="tensorboard",
)

# Trainer ì •ì˜
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
    class_weights=class_weights_tensor
)

# ==============================================================================
# ### 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
# ==============================================================================
print("\n===== [5ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘ =====")
trainer.train()

# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ ì €ì¥
model_save_path = "./my_best_dialect_model_custom_tokenizer"
trainer.save_model(model_save_path)
tokenizer.save_pretrained(model_save_path) # í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸° í¸í•©ë‹ˆë‹¤.

print(f"ìµœì  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ '{model_save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# ==============================================================================
# ### 6ë‹¨ê³„: ìµœì¢… í‰ê°€ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ==============================================================================
print("\n===== [6ë‹¨ê³„] ìµœì¢… í‰ê°€ ë° í…ŒìŠ¤íŠ¸ ì‹œì‘ =====")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í‰ê°€
predictions = trainer.predict(tokenized_test)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids
accuracy = accuracy_score(true_labels, pred_labels)
print(f"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy * 100:.2f}%")


# ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
# ------------------------------------------------------------------------------
# ì €ì¥ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì„œë¹™ í™˜ê²½ê³¼ ìœ ì‚¬)
print("\n--- ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---")
model_path = "./my_best_dialect_model_custom_tokenizer"
loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)
loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)
loaded_model.eval()

sentence = "ê±°ì‹œê¸° ì €ì§ ê°€ë³´ë‘ê»˜"
id2region = {0: "ê°•ì›ë„", 1: "ê²½ìƒë„", 2: "ì „ë¼ë„", 3: "ì œì£¼ë„", 4: "ì¶©ì²­ë„"}

inputs = loaded_tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)

with torch.no_grad():
    outputs = loaded_model(**inputs)
    logits = outputs.logits
    probs = F.softmax(logits, dim=1)
    pred_label = torch.argmax(probs, dim=1).item()

print(f"ğŸ—£ ì…ë ¥ ë¬¸ì¥: {sentence}")
print(f"ğŸ“ ì˜ˆì¸¡ ì§€ì—­: {id2region[pred_label]} (í™•ë¥ : {probs[0][pred_label].item():.2%})")